{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/SNU/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qk7-5P4Rrc-"
      },
      "source": [
        "# Deep Q-network Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEVv-3VNR1zc"
      },
      "source": [
        "If you run in jupyter, turn \n",
        "\n",
        "```\n",
        "colab = False\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "Y9VBDokpR4we",
        "outputId": "0609c133-2a10-4cc7-bc2a-7b9094fd5371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-65.2.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "Successfully installed setuptools-65.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "colab = True\n",
        "if colab:\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsyShng6RrdA",
        "outputId": "b9f23612-3288-443b-8e88-71fd44142f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/snu/8주_RL/강의자료/실습강의/day2/dqn\n",
            "buffer.py     dqn.ipynb        plot.ipynb   setup.ipynb  utils.py\n",
            "day2_dqn.pdf  learning_curves  schedule.py  snapshots\n"
          ]
        }
      ],
      "source": [
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/snu/8주_RL/강의자료/실습강의/day2/dqn\n",
        "    !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92N_slUFRrdB"
      },
      "source": [
        "# -1. Introduction to Gym environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgYlQyPDRrdC"
      },
      "source": [
        "## -1.1 Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WELkYeNcRrdD"
      },
      "source": [
        "# 0. Define Q-network & policy-network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "USpBW4V1RrdE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import MSELoss\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from buffer import ReplayBuffer\n",
        "from utils import save_snapshot, recover_snapshot, load_model\n",
        "from schedule import LinearSchedule\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxWmxxWIRrdF",
        "outputId": "3377cee3-3193-435c-ff9f-18d1a7918c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current device = cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('current device =', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wKSTslpORrdG"
      },
      "outputs": [],
      "source": [
        "# critic network definition\n",
        "# multi-layer perceptron (with 2 hidden layers)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, num_action, hidden_size1, hidden_size2):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, num_action)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        # given a state s, the network returns a vector Q(s,) of length |A|\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        q = self.fc3(x)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAr9QxHNRrdG"
      },
      "source": [
        "# 1. Define DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MGYt9AN1RrdH"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, obs_dim, num_act, hidden1, hidden2):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.num_act = num_act\n",
        "        # networks\n",
        "        self.critic = Critic(obs_dim, num_act, hidden1, hidden2).to(device)\n",
        "                \n",
        "    def act(self, state, epsilon=0.0):\n",
        "        # simple implementation of \\epsilon-greedy method\n",
        "        # TODO : Complete epsilon-greedy action selection\n",
        "        # Hint : np.randon.rand() will generate random number in [0,1]\n",
        "        if np.random.rand() : \n",
        "            return np.random.randint(self.num_act)\n",
        "        else :\n",
        "            # greedy selection\n",
        "            self.critic.eval()\n",
        "            s = torch.Tensor(state).view(1, self.obs_dim).to(device)\n",
        "            q = self.critic(s)\n",
        "            return np.argmax(q.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxyKOc9aRrdH"
      },
      "source": [
        "# 2. Implement one-step param update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wkRLMO0yRrdI"
      },
      "outputs": [],
      "source": [
        "def update(agent, replay_buf, gamma, critic_optim, target_critic, tau, batch_size):\n",
        "    # agent : agent with networks to be trained\n",
        "    # replay_buf : replay buf from which we sample a batch\n",
        "    # actor_optim / critic_optim : torch optimizers\n",
        "    # tau : parameter for soft target update\n",
        "    \n",
        "    agent.critic.train()\n",
        "\n",
        "    batch = replay_buf.sample_batch(batch_size)\n",
        "\n",
        "    # unroll batch\n",
        "    with torch.no_grad():\n",
        "        observations = torch.Tensor(batch['state']).to(device)\n",
        "        actions = torch.tensor(batch['action'], dtype=torch.long).to(device)\n",
        "        rewards = torch.Tensor(batch['reward']).to(device)\n",
        "        next_observations = torch.Tensor(batch['next_state']).to(device)\n",
        "        terminals = torch.Tensor(batch['done']).to(device)\n",
        "\n",
        "        mask = 1.0 - terminals\n",
        "        ### double DQN? ###\n",
        "        # a_inner = torch.unsqueeze(torch.max(agent.critic(next_observations), 1)[1], 1).detach()\n",
        "        # next_q_double = target_critic(observations).gather(1, a_inner)\n",
        "        # next_q_double = mask * next_q_double\n",
        "        ###################\n",
        "        next_q = torch.unsqueeze(target_critic(next_observations).max(1)[0], 1)\n",
        "        next_q = mask * next_q\n",
        "        \n",
        "        # TODO : Build Bellman target for Q-update\n",
        "        target = rewards + gamma * next_q \n",
        "\n",
        "    out = agent.critic(observations).gather(1, actions)\n",
        "\n",
        "    loss_ftn = MSELoss()\n",
        "    loss = loss_ftn(out, target)\n",
        "\n",
        "    critic_optim.zero_grad()\n",
        "    loss.backward()\n",
        "    critic_optim.step()\n",
        "        \n",
        "    # soft target update (both actor & critic network)\n",
        "    for p, targ_p in zip(agent.critic.parameters(), target_critic.parameters()):\n",
        "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
        "        \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZWDysql-RrdI"
      },
      "outputs": [],
      "source": [
        "def evaluate(agent, env, num_episodes=5):\n",
        "\n",
        "    sum_scores = 0.\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        score = 0.\n",
        "        \n",
        "        while not done:\n",
        "            action = agent.act(obs)\n",
        "            obs, rew, done, _ = env.step(action)\n",
        "            score += rew\n",
        "        sum_scores += score\n",
        "    avg_score = sum_scores / num_episodes\n",
        "    \n",
        "    return avg_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx4Um-dtRrdJ"
      },
      "source": [
        "# 3. Combining these, we finally have..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V3I8TRN1RrdJ"
      },
      "outputs": [],
      "source": [
        "def train(agent, env, gamma, \n",
        "          lr, tau,\n",
        "          ep_len, num_updates, batch_size,\n",
        "          init_buffer=5000, buffer_size=100000,\n",
        "          start_train=2000, train_interval=50,\n",
        "          eval_interval=2000, snapshot_interval=10000,\n",
        "          path=None):\n",
        "    \n",
        "    target_critic = copy.deepcopy(agent.critic)\n",
        "    \n",
        "    # environment for evaluation\n",
        "    test_env = copy.deepcopy(env)\n",
        "    # freeze target network\n",
        "    for p in target_critic.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    critic_optim = Adam(agent.critic.parameters(), lr=lr)\n",
        "\n",
        "    if path is not None:\n",
        "        recover_snapshot(path, agent.critic,\n",
        "                         target_critic, critic_optim,\n",
        "                         device=device\n",
        "                        )\n",
        "        # load snapshot\n",
        "    \n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    num_act = env.action_space.n\n",
        "    \n",
        "    replay_buf = ReplayBuffer(obs_dim, buffer_size)\n",
        "    \n",
        "    max_epsilon = 1.\n",
        "    min_epsilon = 0.02\n",
        "    exploration_schedule = LinearSchedule(begin_t=start_train,\n",
        "                                          end_t=num_updates,\n",
        "                                          begin_value=max_epsilon,\n",
        "                                          end_value=min_epsilon\n",
        "                                         )\n",
        "    save_path = './snapshots/'\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    os.makedirs('./learning_curves/', exist_ok=True)\n",
        "    log_file = open('./learning_curves/res.csv',\n",
        "                    'w',\n",
        "                    encoding='utf-8',\n",
        "                    newline=''\n",
        "                   )\n",
        "    logger = csv.writer(log_file)\n",
        "    \n",
        "    # main loop\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    step_count = 0\n",
        "    \n",
        "    for t in range(num_updates + 1):\n",
        "        if t < init_buffer:\n",
        "            # perform random action until we collect sufficiently many samples\n",
        "            # this is for exploration purpose\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # executes epsilon-greedy action\n",
        "            epsilon = exploration_schedule(t)\n",
        "            action = agent.act(obs, epsilon=epsilon)\n",
        "            \n",
        "        next_obs, rew, done, _ = env.step(action)\n",
        "        step_count += 1\n",
        "        if step_count == ep_len:\n",
        "            # if the next_state is not terminal but done is set to True by gym env wrapper\n",
        "            done = False\n",
        "            \n",
        "        replay_buf.append(obs, action, next_obs, rew, done)\n",
        "        obs = next_obs\n",
        "        \n",
        "        if done == True or step_count == ep_len:\n",
        "            # reset environment if current environment reaches a terminal state \n",
        "            # or step count reaches predefined length\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            step_count = 0\n",
        "            # score = evaluate(agent, env)\n",
        "            # print('[iteration {}] evaluation score : {}'.format(t, score))\n",
        "        \n",
        "        if t % eval_interval == 0:\n",
        "            avg_score = evaluate(agent, test_env, num_episodes=5)\n",
        "            print('[iter {}] average score = {} (over 5 episodes)'.format(t, avg_score))\n",
        "            evaluation_log = [t, avg_score]\n",
        "            logger.writerow(evaluation_log)\n",
        "        \n",
        "        if t % snapshot_interval == 0:\n",
        "            snapshot_path = save_path + 'iter{}_'.format(t)\n",
        "            # save weight & training progress\n",
        "            save_snapshot(snapshot_path, agent.critic, target_critic, critic_optim)\n",
        "        \n",
        "        if t > start_train and t % train_interval == 0:\n",
        "            # start training after fixed number of steps\n",
        "            # this may mitigate overfitting of networks to the \n",
        "            # small number of samples collected during the initial stage of training\n",
        "            for _ in range(train_interval):\n",
        "                update(agent,\n",
        "                       replay_buf,\n",
        "                       gamma,\n",
        "                       critic_optim,\n",
        "                       target_critic,\n",
        "                       tau,\n",
        "                       batch_size\n",
        "                      )\n",
        "\n",
        "    log_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlIERr_hRrdK"
      },
      "source": [
        "# 4. Let's train our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvylvSlGRrdL",
        "outputId": "5d84fbee-ba88-4da2-95dc-1cf29b312db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation space dim. : 4 / # actions : 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "num_act = env.action_space.n\n",
        "\n",
        "print('observation space dim. : {} / # actions : {}'.format(obs_dim, num_act))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "62n0MP4lRrdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "430b7b08-b2a4-4ec7-87ee-accddccdc951"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b9946becfbaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Monitor' from 'gym.wrappers' (/usr/local/lib/python3.7/dist-packages/gym/wrappers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "if colab:\n",
        "    import gym\n",
        "    from gym.wrappers import Monitor\n",
        "    import glob\n",
        "    import io\n",
        "    import base64\n",
        "    from IPython.display import HTML\n",
        "    from pyvirtualdisplay import Display\n",
        "    from IPython import display as ipythondisplay\n",
        "\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "\n",
        "    def show_video():\n",
        "      mp4list = glob.glob('video/*.mp4')\n",
        "      if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "      else: \n",
        "        print(\"Could not find video\")\n",
        "        \n",
        "\n",
        "    def wrap_env(env):\n",
        "      env = Monitor(env, './video', force=True)\n",
        "      return env\n",
        "\n",
        "    env = wrap_env(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR3BDfsRRrdL"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(obs_dim=obs_dim, num_act=num_act, hidden1=256, hidden2=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEkgKHOiRrdM"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "lr = 1e-3\n",
        "tau = 1e-3\n",
        "ep_len = 500\n",
        "num_updates = 100000\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u07CLB1IRrdM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train(agent, env, gamma, \n",
        "      lr, tau,\n",
        "      ep_len, num_updates, batch_size,\n",
        "      init_buffer=5000, buffer_size=100000,\n",
        "      start_train=2000, train_interval=50,\n",
        "      eval_interval=2000, snapshot_interval=2000, path=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcnFa4TPRrdM"
      },
      "source": [
        "# 5. Watch the trained agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxSDgFszRrdM"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "if colab:\n",
        "  env = wrap_env(env)\n",
        "obs = env.reset()\n",
        "done = False\n",
        "score = 0.\n",
        "load_model(agent, path='./snapshots/trained.pth.tar', device=device)\n",
        "while not done:\n",
        "    env.render()\n",
        "    obs, rew, done, _ = env.step(agent.act(obs))\n",
        "    score += rew\n",
        "    \n",
        "env.close()\n",
        "print('score : ', score)\n",
        "\n",
        "if colab:\n",
        "    show_video()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dqn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
