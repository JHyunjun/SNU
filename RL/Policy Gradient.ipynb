{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/SNU/blob/main/Policy%20Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJu4KUieWf0g"
      },
      "source": [
        "# REINFORCE and Actor-Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQMZzsSSWf0j"
      },
      "source": [
        "## 0. Setups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StmHu-uWWf0k"
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pY2vLrakWf0k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "\n",
        "# For Colab users, turn this into true\n",
        "colab = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uohFfP8VWf0l"
      },
      "source": [
        "Select hardware to use - GPU or CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-43gLKRwWf0l",
        "outputId": "2750e6f7-b1d4-43d1-d1b0-886c9a031656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpkPfcT5Wf0m"
      },
      "source": [
        "For rendering **[COLAB USE ONLY!]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ISrKQRs0Wf0n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "e82ff0c6-995d-447f-b4c9-df78620d1854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-65.2.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "Successfully installed setuptools-65.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 8.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "\u001b[33mWARNING: gym 0.25.1 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "if colab:\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip3 install box2d-py\n",
        "    !pip3 install gym[Box_2D]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgQW63WRWf0n"
      },
      "source": [
        "Build Environment and check MDP size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kHSGOM0Wf0o",
        "outputId": "ae727741-62f9-408f-b170-c93118722f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of state space / number of actions : 4 / 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:269: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(500)\n",
        "torch.manual_seed(500)\n",
        "\n",
        "# Configure MDP\n",
        "gamma = 0.99\n",
        "state_dim = env.observation_space.low.size\n",
        "num_action = env.action_space.n\n",
        "print('Dimension of state space / number of actions : %d / %d'%(state_dim, num_action))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Yh0of9Wf0o"
      },
      "source": [
        "## 1. Create an policy instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLMkhvVuWf0o"
      },
      "source": [
        " Define policy network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I_FA24WlWf0p"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_dim, num_action, hidden_size1, hidden_size2):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        action_score = self.fc3(x)\n",
        "        return F.softmax(action_score, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yi4u_K3Wf0p"
      },
      "source": [
        "## 2. REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD3U8Z5MWf0p"
      },
      "source": [
        "```python \n",
        "m = Categorial(probs)\n",
        "```\n",
        "makes neural network output computation graph (gradient) into discrete probability distribution, thus it is possible to calculate $\\nabla_\\theta\\log{\\pi_\\theta(a|s)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t3KcYYr2Wf0q"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    state = state.to(device)\n",
        "    probs = pi(state)\n",
        "    \n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    \n",
        "    return action.item(), m.log_prob(action)\n",
        "\n",
        "def sample_trajectory(data, T):\n",
        "    # Reset environment to get new trajectory\n",
        "    state = env.reset()\n",
        "    r_sum, r_sum_discount = 0, 0\n",
        "    \n",
        "    for t in range(T):\n",
        "        # Get action from current policy and rollout\n",
        "        action, log_prob = select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        r_sum += reward\n",
        "        r_sum_discount += reward * (gamma ** t) \n",
        "\n",
        "        # Store data\n",
        "        data['log_pi'].append(-log_prob) # (-) sign for gradient ascent\n",
        "        data['state'].append(state)\n",
        "        data['next_state'].append(next_state)\n",
        "        data['reward'].append(reward)\n",
        "        \n",
        "        # Step\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return r_sum, r_sum_discount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx230Kg4Wf0q"
      },
      "source": [
        "REINFORCE algorithm approximate gradient for policy parameter $\\theta$ with sampled trajectory\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\big( \\sum^T_{t=0}\\nabla_\\theta\\log{\\pi_\\theta}(a_t|s_t) \\big) \\big( \\sum^T_{t=0} \\gamma^t r(s_t,a_t) \\big)$$\n",
        "\n",
        "\n",
        "With further approximation and use of baseline,\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=0} \\big( \\nabla_\\theta\\log{\\pi_\\theta(a_t|s_t)} \\big) \\big( Q(s_t,a_t) - v(s_t) \\big)$$\n",
        "\n",
        "\n",
        "For REINFORCE, we use \n",
        "$$Q(s_t, a_t) \\approx \\sum^T_{t'=t} \\gamma^t r(s_{t'}, a_{t'})$$\n",
        "\n",
        "$$v^{\\pi_\\theta}(s_0) \\approx \\sum^N_{i=0} \\sum^T_{t=0} \\gamma^t r^i(s_t, a_t)$$ \n",
        "\n",
        "(Note that we use universial baseline!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JyyaPxk-Wf0q"
      },
      "outputs": [],
      "source": [
        "def calculate_PG(pi_returns_discounted, dataset):\n",
        "    pi_loss = 0\n",
        "    for data in dataset:\n",
        "        advantage, DCR = [], 0\n",
        "        for r in reversed(data['reward']):\n",
        "            # TODO : Caculate discounted return from t=i\n",
        "            # Hint : reversed() will give saved rewards in reversed order\n",
        "            # r_T, r_{T-1}, ... r_0\n",
        "            # DCR(T) = r_T\n",
        "            # DCR(T-1) = r_{T-1} + gamma * r_T\n",
        "            DCR = r + gamma * DCR\n",
        "            \n",
        "            # Q(s,a) is replaced with discounted sum of rewards (DCR)\n",
        "            # v(s) is replaced with empirical v(s_0)\n",
        "            advantage.insert(0, DCR - np.mean(pi_returns_discounted))\n",
        "\n",
        "        # TODO : alternate between two losses to see difference!\n",
        "        # pi_loss_vanilla = [log_pi * DCR for log_pi in data['log_pi']]\n",
        "        pi_loss_baseline = [log_pi * a for log_pi, a in zip(data['log_pi'], advantage)]\n",
        "        \n",
        "        # Take mean value\n",
        "        pi_loss += torch.cat(pi_loss_vanilla).sum()\n",
        "        \n",
        "    return pi_loss / num_trajs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uTJzK7iiWf0q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "a1d92e43-74ac-41af-daf7-c62a62bcec1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d07d3ccd9e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Perform pocliy gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpi_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_PG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi_returns_discounted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mpi_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moptimizer_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3b1c09bd3ff2>\u001b[0m in \u001b[0;36mcalculate_PG\u001b[0;34m(pi_returns_discounted, dataset)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Take mean value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpi_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi_loss_vanilla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpi_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_trajs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pi_loss_vanilla' is not defined"
          ]
        }
      ],
      "source": [
        " num_epochs = 100 # Num. of gradient steps\n",
        "num_trajs = 100 # N\n",
        "T = 10000 # T\n",
        "log_interval = 5\n",
        "total_time = []\n",
        "\n",
        "pi = Policy(state_dim, num_action, 128, 128).to(device)\n",
        "optimizer_pi = optim.Adam(pi.parameters(), lr=1e-3)\n",
        "\n",
        "# For logging\n",
        "pi_returns, pi_returns_discounted = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_epoch = time.time()\n",
        "    \n",
        "    # On-policy dataset\n",
        "    # We cannot reuse samples!\n",
        "    dataset = []\n",
        "    \n",
        "    # Collect trajectories to perform gradient step\n",
        "    for N in range(num_trajs):\n",
        "        data = {'log_pi':[], 'state':[], 'next_state':[], 'reward':[]}\n",
        "        r_sum, r_sum_discount = sample_trajectory(data, T)\n",
        "        dataset.append(data)\n",
        "\n",
        "        # For logging - store most recent N trajectories\n",
        "        pi_returns.append(r_sum)\n",
        "        pi_returns_discounted.append(r_sum_discount)\n",
        "        if len(pi_returns) > num_trajs:\n",
        "            pi_returns.pop(0)\n",
        "            pi_returns_discounted.pop(0)\n",
        "    \n",
        "    # Perform pocliy gradient step\n",
        "    optimizer_pi.zero_grad()\n",
        "    pi_loss = calculate_PG(pi_returns_discounted, dataset)\n",
        "    pi_loss.backward()\n",
        "    optimizer_pi.step()\n",
        "    \n",
        "    # Logging - print most recent epoch result\n",
        "    epoch_time = time.time() - start_epoch\n",
        "    total_time.append(epoch_time)\n",
        "    if epoch % log_interval == 0:\n",
        "        time_elapsed = np.sum(total_time)\n",
        "        time_remain = np.mean(total_time) * num_epochs - time_elapsed\n",
        "        print('Epoch {}\\tReturn_mean: {:.2f}\\tReturn_std: {:.2f}\\tTime(Elapsed/Remain): {:.2f}/{:.2f} (mins)'.format(\n",
        "            epoch, np.mean(pi_returns), np.std(pi_returns), time_elapsed/60, time_remain/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvM6RFyX2gmS"
      },
      "source": [
        "## 3. Actor-Critic (Linear Architecture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgnEVIG_Wf0r"
      },
      "outputs": [],
      "source": [
        "from numpy import linalg as LA\n",
        "\n",
        "# Calculate feature vector\n",
        "# state[0] : Cart pos\n",
        "# state[1] : Cart speed\n",
        "# state[2] : Pole angle\n",
        "# state[3] : Pole velocity at tip\n",
        "\n",
        "state2 = [-0.12, 0, 0.12] # termination condition\n",
        "state3 = [-1, 0, 1]\n",
        "\n",
        "mu = []\n",
        "for s2 in state2:\n",
        "    for s3 in state3:\n",
        "        mu.append([s2, s3])\n",
        "\n",
        "def state2feature(state):\n",
        "    phi = []\n",
        "    for f in mu:\n",
        "        rad_base = LA.norm(np.array(state[-2:])-np.array(f)) ** 2\n",
        "        phi.append(np.exp(-0.5*rad_base))\n",
        "    return np.array(phi)\n",
        "\n",
        "\n",
        "def calculate_vf(dataset, vf):\n",
        "    X, y = [], []\n",
        "    \n",
        "    for data in dataset:\n",
        "        for s, next_s, r in zip(data['state'], data['next_state'], data['reward']):\n",
        "            v = state2feature(s)\n",
        "            Q = r\n",
        "            if vf is not None:\n",
        "                Q = r + gamma * vf.predict(state2feature(next_s).reshape(1, -1))[0]\n",
        "            X.append(v)\n",
        "            y.append(Q)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "\n",
        "def get_advantage(data, vf):\n",
        "    advantage, baseline = [], []\n",
        "    \n",
        "    for s, next_s, r in zip(data['state'], data['next_state'], data['reward']):\n",
        "        v = vf.predict(state2feature(s).reshape(1, -1))[0]\n",
        "        v_next = vf.predict(state2feature(next_s).reshape(1, -1))[0]\n",
        "        # TODO: Complete advantage calculation by calculating Q-value\n",
        "        # Hint: SARSA style!\n",
        "        Q = r + gamma * v_next - v\n",
        "        A = Q - v\n",
        "        \n",
        "        advantage.append(A)\n",
        "        baseline.append(v)\n",
        "    \n",
        "    return advantage, baseline\n",
        "\n",
        "\n",
        "def calculate_AC_PG(vf, pi_returns_discounted, dataset):\n",
        "    pi_loss = 0\n",
        "    for data in dataset:\n",
        "        # For linear Actor-Critic\n",
        "        advantage = []\n",
        "        _, v = get_advantage(data, vf)\n",
        "        DCR = 0\n",
        "        for i, r in enumerate(reversed(data['reward'])):\n",
        "            DCR = r + gamma * DCR\n",
        "            advantage.insert(0, DCR - v[i]) # For practical algorithm, we just adopt baseline\n",
        "\n",
        "        # Compute each element of gradient\n",
        "        pi_loss_linear_vf = [log_pi * a for log_pi, a in zip(data['log_pi'], advantage)]\n",
        "        \n",
        "        # Sums up log_prob * weight\n",
        "        pi_loss += torch.cat(pi_loss_linear_vf).sum()\n",
        "        \n",
        "    return pi_loss / num_trajs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjUVMwGDWf0s"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "num_trajs = 100\n",
        "T = 10000\n",
        "log_interval = 5\n",
        "total_time = []\n",
        "\n",
        "pi = Policy(state_dim, num_action, 128, 128).to(device)\n",
        "optimizer_pi = optim.Adam(pi.parameters(), lr=1e-3)\n",
        "vf = None\n",
        "\n",
        "# For logging\n",
        "pi_returns, pi_returns_discounted = [], []\n",
        "\n",
        "dataset_vf = []\n",
        "for epoch in range(num_epochs):\n",
        "    start_epoch = time.time()\n",
        "    \n",
        "    # On-policy dataset\n",
        "    dataset = []\n",
        "    \n",
        "    # Collect trajectories to perform gradient step\n",
        "    for N in range(num_trajs):\n",
        "        data = {'log_pi':[], 'state':[], 'next_state':[], 'reward':[]}\n",
        "        r_sum, r_sum_discount = sample_trajectory(data, T)\n",
        "        dataset.append(data)\n",
        "        dataset_vf.append(data)\n",
        "\n",
        "        # For logging - store most recent N trajectories\n",
        "        pi_returns.append(r_sum)\n",
        "        pi_returns_discounted.append(r_sum_discount)\n",
        "        if len(pi_returns) > num_trajs:\n",
        "            pi_returns.pop(0)\n",
        "            pi_returns_discounted.pop(0)\n",
        "\n",
        "    ### NEW : update critic ###\n",
        "    X, y = calculate_vf(dataset_vf, vf)\n",
        "    vf = LinearRegression().fit(X, y)\n",
        "    \n",
        "    # Perform pocliy gradient step\n",
        "    optimizer_pi.zero_grad()\n",
        "    pi_loss = calculate_AC_PG(vf, pi_returns_discounted, dataset)\n",
        "    pi_loss.backward()\n",
        "    optimizer_pi.step()\n",
        "    \n",
        "    # Logging - print most recent epoch result\n",
        "    epoch_time = time.time() - start_epoch\n",
        "    total_time.append(epoch_time)\n",
        "    if epoch % log_interval == 0:\n",
        "        dataset_vf = []\n",
        "        time_elapsed = np.sum(total_time)\n",
        "        time_remain = np.mean(total_time) * num_epochs - time_elapsed\n",
        "        print('Epoch {}\\tReturn_mean: {:.2f}\\tReturn_std: {:.2f}\\tTime(Elapsed/Remain): {:.2f}/{:.2f} (mins)'.format(\n",
        "            epoch, np.mean(pi_returns), np.std(pi_returns), time_elapsed/60, time_remain/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYAym5jgWf0s"
      },
      "source": [
        "## 4. Visualize result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCfJmHFyWf0t"
      },
      "source": [
        "For rendering **[COLAB USE ONLY!]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy_rQDIOWf0t"
      },
      "outputs": [],
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "            loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env\n",
        "\n",
        "if colab:\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "\n",
        "    env = wrap_env(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFet5p_6Wf0t"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "if colab:\n",
        "  env = wrap_env(env)\n",
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "    action, log_prob = select_action(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if done: \n",
        "        break\n",
        "            \n",
        "env.close()\n",
        "if colab:\n",
        "    show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pg_ac.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
