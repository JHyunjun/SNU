{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trpo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/SNU/blob/main/TRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOLkEzexcvHm"
      },
      "source": [
        "# Trust Region Policy Optimization Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nTDAymrIVwI"
      },
      "source": [
        "# -1. Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXX4sZB3c0SP"
      },
      "source": [
        "If you run in jupyter, turn \n",
        "\n",
        "```\n",
        "colab = False\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "HEtOZ-YkcwJG",
        "outputId": "fad2bd23-7805-4bdf-cdae-62030163257d"
      },
      "source": [
        "colab = True\n",
        "if colab:\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip3 install box2d-py\n",
        "    !pip3 install gym[Box_2D]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-65.2.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "Successfully installed setuptools-65.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "\u001b[33mWARNING: gym 0.25.1 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (3.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HQocIk1dO8E",
        "outputId": "a0273409-1de5-4d36-a15c-73bc7b3223fa"
      },
      "source": [
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/snu/8주_RL/강의자료/실습강의/day4/trpo\n",
        "    !ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/snu/8주_RL/강의자료/실습강의/day4/trpo\n",
            "day4_trpo_ppo.pdf  ppo_learning_curves\t__pycache__  utils.py\n",
            "learning_curves    ppo.py\t\tsnapshots    video\n",
            "memory.py\t   ppo_snapshots\ttrpo.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5TY81puqgyS",
        "outputId": "68593da6-e6ea-4d09-c812-3efd386f1194"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pnRhmQQcvHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db899a76-fd73-4292-b4bb-582a4b868790"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "import torch\n",
        "import os\n",
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Independent\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.optim import Adam\n",
        "from memory import OnPolicyMemory\n",
        "from utils import *"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/snu/8주_RL/강의자료/실습강의/day4/trpo/utils.py:87: DeprecationWarning: invalid escape sequence \\p\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1ndOboccvHo",
        "outputId": "0990b16a-5b2c-4c2f-d9c7-600c88f631db"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('current device : ', device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current device :  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0R0gU2pcvHp"
      },
      "source": [
        "# 0. Network Architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX1R4ZqNcvHq"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
        "        # actor f_\\phi(s)\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden2, act_dim)  # for \\mu\n",
        "        self.fc4 = nn.Linear(hidden2, act_dim)  # for \\sigma\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = torch.tanh(self.fc1(obs))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "\n",
        "        mu = self.fc3(x)\n",
        "        log_sigma = self.fc4(x)\n",
        "\n",
        "        sigma = torch.exp(log_sigma)\n",
        "\n",
        "        return mu, sigma\n",
        "\n",
        "    def log_prob(self, obs, act):\n",
        "        mu, sigma = self.forward(obs)\n",
        "        act_distribution = Independent(Normal(mu, sigma), 1)\n",
        "        log_prob = act_distribution.log_prob(act)\n",
        "        return log_prob\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    # critic V(s ; \\theta)\n",
        "    def __init__(self, obs_dim, hidden1, hidden2):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, 1)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = torch.tanh(self.fc1(obs))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "\n",
        "        return self.fc3(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4h65Dy-cvHq"
      },
      "source": [
        "# 1. Agent Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-oN94SZcvHr"
      },
      "source": [
        "class TRPOAgent:\n",
        "    def __init__(\n",
        "                 self,\n",
        "                 obs_dim,\n",
        "                 act_dim,\n",
        "                 hidden1=64,\n",
        "                 hidden2=32,\n",
        "                 ):\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.hidden1 = hidden1\n",
        "        self.hidden2 = hidden2\n",
        "\n",
        "        self.pi = Actor(obs_dim, act_dim, hidden1, hidden2).to(device)\n",
        "        self.V = Critic(obs_dim, hidden1, hidden2).to(device)\n",
        "\n",
        "    def act(self, obs, deterministic=False):\n",
        "        obs = torch.tensor(obs, dtype=torch.float).to(device)\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = self.pi(obs)\n",
        "            if deterministic:\n",
        "                action = mu\n",
        "                log_prob = None\n",
        "                val = None\n",
        "            else:\n",
        "                act_distribution = Independent(Normal(mu, sigma), 1)\n",
        "                action = act_distribution.sample()\n",
        "                log_prob = act_distribution.log_prob(action)\n",
        "                val = self.V(obs)\n",
        "                log_prob = log_prob.cpu().numpy()\n",
        "                val = val.cpu().numpy()\n",
        "\n",
        "        action = action.cpu().numpy()\n",
        "        \n",
        "\n",
        "        return action, log_prob, val"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xweZIpcicvHr"
      },
      "source": [
        "# 2. Policy & Value Function Approximation Update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aJyKaVEyds8"
      },
      "source": [
        "Objective:\n",
        "\\begin{align*}\n",
        "g = \\nabla_\\phi J(\\phi) &\\approx \\nabla_\\phi \\mathbb{E}_{s \\sim \\rho_{\\phi_{\\text{old}}}, a \\sim \\pi_{\\phi_{\\text{old}}}}\\left( \\frac{\\pi_{\\phi}(s, a)}{\\pi_{\\phi_{\\text{old}}}(s, a)} A^{\\pi_{\\phi_{\\text{old}}}}(s, a) \\right) \\\\\n",
        "&\\approx \\nabla_\\phi \\frac{1}{N} \\sum_{i = 1}^N \\left( \\frac{\\pi_{\\phi}(s_i, a_i)}{\\pi_{\\phi_{\\text{old}}}(s_i, a_i)} \\hat A(s_i, a_i) \\right).\n",
        "\\end{align*} \\\\\n",
        "Since we take into account approximated trust region constraint, the final update direction is\n",
        "\\begin{equation*}\n",
        "s = H^{-1}g, \\quad H s = g,\n",
        "\\end{equation*}\n",
        " and the stepsize is\n",
        " \\begin{equation*}\n",
        "\\alpha = \\sqrt{\\frac{2\\delta}{g^\\top H^{-1} g}}.\n",
        " \\end{equation*}\n",
        " Thus, the update is done as follows:\n",
        " \\begin{gather*}\n",
        " \\phi_{\\text{old}} \\longleftarrow \\phi, \\\\\n",
        "\\phi \\longleftarrow \\phi + \\alpha \\cdot s.\n",
        " \\end{gather*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nowAebz5cvHs"
      },
      "source": [
        "def update(agent, memory, critic_optim, delta, num_updates):\n",
        "    \n",
        "    batch = memory.load()\n",
        "\n",
        "    states = torch.Tensor(batch['state']).to(device)\n",
        "    actions = torch.Tensor(batch['action']).to(device)\n",
        "    target_v = torch.Tensor(batch['val']).to(device)\n",
        "    A = torch.Tensor(batch['A']).to(device)\n",
        "    old_log_probs = torch.Tensor(batch['log_prob']).to(device)\n",
        "    \n",
        "    for _ in range(num_updates):\n",
        "        ################\n",
        "        # train critic #\n",
        "        ################\n",
        "        out = agent.V(states)\n",
        "        critic_loss = torch.mean((out - target_v)**2)\n",
        "\n",
        "        critic_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        critic_optim.step()\n",
        "\n",
        "        ###################\n",
        "        # policy gradient #\n",
        "        ###################\n",
        "        log_probs = agent.pi.log_prob(states, actions)\n",
        "\n",
        "        # TODO : calculate below to get probabiltiy ratio\n",
        "        # Hint : use log_probs and old_log_probs\n",
        "        # \\pi(a_t | s_t ; \\phi) / \\pi(a_t | s_t ; \\phi_old)\n",
        "        #prob_ratio = torch.exp()\n",
        "        prob_ratio = torch.exp(log_probs - old_log_probs)\n",
        "\n",
        "        actor_loss = torch.mean(prob_ratio * A)\n",
        "        loss_grad = torch.autograd.grad(actor_loss, agent.pi.parameters())\n",
        "        # flatten gradients of params\n",
        "        g = torch.cat([grad.view(-1) for grad in loss_grad]).data\n",
        "\n",
        "        s = cg(fisher_vector_product, g, agent.pi, states)\n",
        "\n",
        "        sAs = torch.sum(fisher_vector_product(s, agent.pi, states) * s, dim=0, keepdim=True)\n",
        "        step_size = torch.sqrt(2 * delta / sAs)[0]    # stepsize : move as far as possible within trust region\n",
        "        step = step_size * s\n",
        "\n",
        "        old_actor = Actor(agent.obs_dim, agent.act_dim, agent.hidden1, agent.hidden2).to(device)\n",
        "        old_actor.load_state_dict(agent.pi.state_dict())\n",
        "\n",
        "        params = flat_params(agent.pi)\n",
        "\n",
        "        backtracking_line_search(old_actor, agent.pi, actor_loss, g,\n",
        "                                 old_log_probs, params, step, delta, A, states, actions)    # line search => for improvement guarantee!\n",
        "    return"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpuMGH3fcvHt"
      },
      "source": [
        "def evaluate(agent, env, num_episodes=5):\n",
        "\n",
        "    scores = np.zeros(num_episodes)\n",
        "    for i in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        score = 0.\n",
        "        while not done:\n",
        "            action = agent.act(obs, deterministic=True)[0]\n",
        "            obs, rew, done, _ = env.step(action)\n",
        "            score += rew\n",
        "\n",
        "        scores[i] = score\n",
        "    avg_score = np.mean(scores)\n",
        "    std_score = np.std(scores)\n",
        "    return avg_score, std_score"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfzZ3SU-cvHt"
      },
      "source": [
        "# 3. Training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLzSmhDocvHu"
      },
      "source": [
        "def train(env, agent, max_iter, gamma=0.99, lr=3e-4, lam=0.95, delta=1e-3, steps_per_epoch=10000, eval_interval=10000, snapshot_interval=10000):\n",
        "    \n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "    max_ep_len = env._max_episode_steps\n",
        "    memory = OnPolicyMemory(obs_dim, act_dim, gamma, lam, lim=steps_per_epoch)\n",
        "    test_env = copy.deepcopy(env)\n",
        "    critic_optim = Adam(agent.V.parameters(), lr=lr)\n",
        "\n",
        "    save_path = './snapshots/'\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    os.makedirs('./learning_curves/', exist_ok=True)\n",
        "    log_file = open('./learning_curves/res.csv',\n",
        "                    'w',\n",
        "                    encoding='utf-8',\n",
        "                    newline=''\n",
        "                   )\n",
        "    logger = csv.writer(log_file)\n",
        "    num_epochs = max_iter // steps_per_epoch\n",
        "    total_t = 0\n",
        "    begin = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        # start agent-env interaction\n",
        "        state = env.reset()\n",
        "        step_count = 0\n",
        "        ep_reward = 0\n",
        "\n",
        "        for t in range(steps_per_epoch):\n",
        "            # collect transition samples by executing the policy\n",
        "            action, log_prob, v = agent.act(state)\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            memory.append(state, action, reward, v, log_prob)\n",
        "\n",
        "            ep_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if (step_count == max_ep_len) or (t == steps_per_epoch - 1):\n",
        "                # termination of env by env wrapper, or by truncation due to memory size\n",
        "                s_last = torch.tensor(next_state, dtype=torch.float).to(device)\n",
        "                v_last = agent.V(s_last).item()\n",
        "                memory.compute_values(v_last)\n",
        "            elif done:\n",
        "                # episode done as the agent reach a terminal state\n",
        "                v_last = 0.0\n",
        "                memory.compute_values(v_last)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                step_count = 0\n",
        "                ep_reward = 0\n",
        "\n",
        "            if total_t % eval_interval == 0:\n",
        "                avg_score, std_score = evaluate(agent, test_env, num_episodes=5)\n",
        "                elapsed_t = time.time() - begin\n",
        "                print('[elapsed time : {:.1f}s| iter {}] score = {:.2f}'.format(elapsed_t, total_t, avg_score), u'\\u00B1', '{:.4f}'.format(std_score))\n",
        "                evaluation_log = [t, avg_score, std_score]\n",
        "                logger.writerow(evaluation_log)\n",
        "\n",
        "\n",
        "            if total_t % snapshot_interval == 0:\n",
        "                snapshot_path = save_path + 'iter{}_'.format(total_t)\n",
        "                # save weight & training progress\n",
        "                save_snapshot(agent, snapshot_path)\n",
        "\n",
        "            total_t += 1\n",
        "\n",
        "        # train agent at the end of each epoch\n",
        "        update(agent, memory, critic_optim, delta, num_updates=1)\n",
        "    log_file.close()\n",
        "    return"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcUXG4EfGt0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae575216-c260-48f2-d67b-1086d053f41f"
      },
      "source": [
        "# Let's move to robotic environment!\n",
        "!pip install pybullet"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 27 kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQr9w8lcvHu",
        "outputId": "4f372b86-7ae8-4938-f333-84efef2f45b1"
      },
      "source": [
        "import pybullet_envs\n",
        "\n",
        "env_id = 'HopperBulletEnv-v0'\n",
        "\n",
        "env = gym.make(env_id)\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "print('observation space dim. : {} / action space dim. : {}'.format(obs_dim, act_dim))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation space dim. : 15 / action space dim. : 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:441: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  \"The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS6XCK63cvHv"
      },
      "source": [
        "agent = TRPOAgent(obs_dim, act_dim, hidden1=128, hidden2=128)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubINos42Bqt6",
        "outputId": "947bee88-8323-497c-edc2-ff1ff2733988"
      },
      "source": [
        "next(agent.pi.parameters()).is_cuda"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K0q2WTDcvHv"
      },
      "source": [
        "#train(env, agent, max_iter=20000000, gamma=0.99, lr=5e-4, lam=0.95, delta=1e-3, steps_per_epoch=10000, eval_interval=500000)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZBv4n0dcvHv"
      },
      "source": [
        "# 4. Watch how your agent solve the task!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxeYWf1EcvHw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "2a8f4d04-d8cf-4eca-b6d4-6fa5b16836a8"
      },
      "source": [
        "if colab:\n",
        "    import gym\n",
        "    from gym.wrappers import Monitor\n",
        "    import glob\n",
        "    import io\n",
        "    import base64\n",
        "    from IPython.display import HTML\n",
        "    from pyvirtualdisplay import Display\n",
        "    from IPython import display as ipythondisplay\n",
        "\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "\n",
        "    def show_video():\n",
        "      mp4list = glob.glob('video/*.mp4')\n",
        "      if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "      else: \n",
        "        print(\"Could not find video\")\n",
        "        \n",
        "\n",
        "    def wrap_env(env):\n",
        "      env = Monitor(env, './video', force=True)\n",
        "      return env\n",
        "\n",
        "    env = wrap_env(env)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b9946becfbaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Monitor' from 'gym.wrappers' (/usr/local/lib/python3.7/dist-packages/gym/wrappers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoEoj-R-cvHw"
      },
      "source": [
        "env = gym.make('HopperBulletEnv-v0')\n",
        "if colab:\n",
        "  env = wrap_env(env)\n",
        "\n",
        "load_model(agent, './snapshots/hopper_expert.tar', device)\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "done = False\n",
        "score = 0.\n",
        "while not done:\n",
        "    env.render()\n",
        "    obs, rew, done, _ = env.step(agent.act(obs, deterministic=True)[0])\n",
        "    score += rew\n",
        "env.close()\n",
        "print('score : ', score)\n",
        "\n",
        "if colab:\n",
        "  show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiZ1VUHUHtfd"
      },
      "source": [
        "# Proximal Policy Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U63srv3xcZE"
      },
      "source": [
        "In contrast to TRPO, PPO uses the following simple $1^{\\text{st}}$-order objective!\n",
        "\\begin{equation*}\n",
        "L(\\phi) \\approx \\frac{1}{N} \\sum_{i = 1}^N \\min\\left( r_i(\\phi)\\hat A_i, \\text{clip}(r_i(\\phi), 1 - \\varepsilon, 1 + \\varepsilon) \\hat A_i  \\right).\n",
        "\\end{equation*}\n",
        "While we performed complex parameter updates in TRPO, we just build the above loss and use popular optimizers provided by PyTorch..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXmOA2yWHs2l"
      },
      "source": [
        "from ppo import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OCKvpG4RSS3"
      },
      "source": [
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "print('observation space dim. : {} / action space dim. : {}'.format(obs_dim, act_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeUrio16HxSE"
      },
      "source": [
        "ppo_agent = PPOAgent(obs_dim, act_dim, hidden1=128, hidden2=128, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WQ8D5BiNmfd"
      },
      "source": [
        "ppo_train(env, ppo_agent, max_iter=500000, gamma=0.99, lr=3e-4, lam=0.95, epsilon=0.2, steps_per_epoch=10000, eval_interval=10000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
