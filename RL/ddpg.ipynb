{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "ddpg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/SNU/blob/main/ddpg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuuPbvbbwXd3"
      },
      "source": [
        "# -1. Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_r6QEqZz66"
      },
      "source": [
        "If you run in jupyter, turn \n",
        "\n",
        "```\n",
        "colab = False\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnK9tUehZqJ0",
        "outputId": "9bfd058f-cd5e-499d-c777-f7df35032265"
      },
      "source": [
        "colab = True\n",
        "if colab:\n",
        "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip3 install box2d-py\n",
        "    !pip3 install gym[Box_2D]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (65.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "\u001b[33mWARNING: gym 0.25.1 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgztXeWhaDfB",
        "outputId": "afc15145-fb89-40ca-98b5-89cb2662857a"
      },
      "source": [
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/Colab Notebooks/snu/8주_RL/강의자료/실습강의/day3/ddpg\n",
        "    !ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/snu/8주_RL/강의자료/실습강의/day3/ddpg\n",
            "buffer.py  day3_ddpg.pdf  ddpg.ipynb  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVIk7olNhmT4",
        "outputId": "4a981402-7ad4-4fbb-9838-2c64a221c6d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raPrcmjpZePo"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import MSELoss\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from buffer import ReplayBuffer\n",
        "from utils import save_snapshot, recover_snapshot, load_model\n",
        "import gym"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEmB9tRLZePr",
        "outputId": "b0be9fe2-e281-4082-9fbf-af8c6d70c43a"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('current device =', device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current device = cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5dTtQn-ZePs"
      },
      "source": [
        "# 0. Define Q-network & policy-network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTHHxIokZePt"
      },
      "source": [
        "# critic network definition\n",
        "# multi-layer perceptron (with 2 hidden layers)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_dim + act_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, 1)\n",
        "        \n",
        "    \n",
        "    def forward(self, obs, act):\n",
        "        x = torch.cat([obs, act], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        \n",
        "        return self.fc3(x)\n",
        "    \n",
        "    \n",
        "# actor network definition\n",
        "# multi-layer perceptron (with 2 hidden layers)\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, act_dim)\n",
        "        self.ctrl_range = ctrl_range\n",
        "        \n",
        "    def forward(self, obs):\n",
        "        x = F.relu(self.fc1(obs))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        \n",
        "        return self.ctrl_range * torch.tanh(self.fc3(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKvObdk4ZePu"
      },
      "source": [
        "# 1. Define DDPG agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pZYZQr1ZePu"
      },
      "source": [
        "class DDPGAgent:\n",
        "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
        "        \n",
        "        # networks\n",
        "        self.actor = Actor(obs_dim, act_dim, ctrl_range, hidden1, hidden2).to(device)\n",
        "        self.critic = Critic(obs_dim, act_dim, hidden1, hidden2).to(device)\n",
        "                \n",
        "    def act(self, obs):\n",
        "        # numpy ndarray to torch tensor\n",
        "        # we first add an extra dimension\n",
        "        obs = obs[np.newaxis, ...]\n",
        "        with torch.no_grad():\n",
        "            obs_tensor = torch.Tensor(obs).to(device)\n",
        "            # TODO : get agent action from policy network (self.actor)\n",
        "            act_tensor = self.actor(obs_tensor)\n",
        "\n",
        "        # torch tensor to numpy ndarray\n",
        "        # remove extra dimension\n",
        "        action = act_tensor.cpu().detach().numpy()\n",
        "        action = np.squeeze(action, axis=0)\n",
        "        \n",
        "        return action\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReax398ZePv"
      },
      "source": [
        "## 1.1.Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dycWpSbZePw",
        "outputId": "dd32ef80-0f60-44dc-be8c-a382273a81ce"
      },
      "source": [
        "agent = DDPGAgent(4, 2, 3, 32, 32)\n",
        "action = agent.act(np.array([3., -1., 2., -5.]))\n",
        "print(action)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0799744  0.01371256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJE6704wZePw"
      },
      "source": [
        "# 2. Implement one-step param update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gn6x3KpsKps"
      },
      "source": [
        "Actor update? \\\\\n",
        "First Observe\n",
        "\\begin{equation*}\n",
        "\\nabla_{\\phi} Q_{\\theta}(s_t, \\mu_{\\phi}(s_t)) =  \\nabla_{\\phi} \\mu_{\\phi}(s_t) \\cdot \\nabla_{a} Q(s_t, a)|_{a={\\mu_{\\phi}(s)}}.\n",
        "\\end{equation*}\n",
        "Thus, we have \n",
        "\\begin{equation*}\n",
        "\\nabla_\\phi J(\\phi) \\approx \\frac{1}{N}\\sum_{i = 1}^N \\nabla_{\\phi} \\mu_{\\phi}(s_t) \\cdot \\nabla_{a} Q(s_t, a)|_{a={\\mu_{\\phi}(s)}}  = \\nabla_\\phi\\left( \\frac{1}{N}\\sum_{i = 1}^N Q_{\\theta}(s_t, \\mu_{\\phi}(s_t)) \\right).\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K75e7LJpZePx"
      },
      "source": [
        "def update(agent, replay_buf, gamma, actor_optim, critic_optim, target_actor, target_critic, tau, batch_size):\n",
        "    # agent : agent with networks to be trained\n",
        "    # replay_buf : replay buf from which we sample a batch\n",
        "    # actor_optim / critic_optim : torch optimizers\n",
        "    # tau : parameter for soft target update\n",
        "    \n",
        "    batch = replay_buf.sample_batch(batch_size=batch_size)\n",
        "\n",
        "    # target construction does not need backward ftns\n",
        "    with torch.no_grad():\n",
        "        # unroll batch\n",
        "        obs = torch.Tensor(batch.obs).to(device)\n",
        "        act = torch.Tensor(batch.act).to(device)\n",
        "        next_obs = torch.Tensor(batch.next_obs).to(device)\n",
        "        rew = torch.Tensor(batch.rew).to(device)\n",
        "        done = torch.Tensor(batch.done).to(device)\n",
        "        \n",
        "        ################\n",
        "        # train critic #\n",
        "        ################\n",
        "        mask = 1. - done\n",
        "        target = rew + gamma * mask * target_critic(next_obs, target_actor(next_obs))\n",
        "    \n",
        "    out = agent.critic(obs, act)\n",
        "\n",
        "    # TODO : Build critic MSELoss by yourself!\n",
        "    # Hint : use torch.mean\n",
        "    critic_loss = torch.mean((out - target)**2)\n",
        "    \n",
        "    \n",
        "    critic_optim.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optim.step()\n",
        "    \n",
        "    ###############\n",
        "    # train actor #\n",
        "    ###############\n",
        "    \n",
        "    # freeze critic during actor training (why?)\n",
        "    for p in agent.critic.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    \n",
        "    # TODO : actor loss construction! (Warning: sign of the loss?)\n",
        "    actor_loss = -torch.mean(agent.critic(obs, agent.actor(obs)))\n",
        "    \n",
        "    actor_optim.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optim.step()\n",
        "    \n",
        "    \n",
        "    # unfreeze critic after actor training\n",
        "    for p in agent.critic.parameters():\n",
        "        p.requires_grad_(True)\n",
        "        \n",
        "    # soft target update (both actor & critic network)\n",
        "    for p, targ_p in zip(agent.actor.parameters(), target_actor.parameters()):\n",
        "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
        "    for p, targ_p in zip(agent.critic.parameters(), target_critic.parameters()):\n",
        "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
        "        \n",
        "        \n",
        "    return"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQh-cLQ0ZePy"
      },
      "source": [
        "def evaluate(agent, env, num_episodes=5):\n",
        "\n",
        "    sum_scores = 0.\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        score = 0.\n",
        "        \n",
        "        while not done:\n",
        "            action = agent.act(obs)\n",
        "            obs, rew, done, _ = env.step(action)\n",
        "            score += rew\n",
        "        sum_scores += score\n",
        "    avg_score = sum_scores / num_episodes\n",
        "    \n",
        "    return avg_score"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrosDS4ZZePy"
      },
      "source": [
        "# 3. Combining these, we finally have..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sQTm_8RZePy"
      },
      "source": [
        "def train(agent, env, gamma, \n",
        "          actor_lr, critic_lr, tau, noise_std,\n",
        "          ep_len, num_updates, batch_size,\n",
        "          init_buffer=5000, buffer_size=100000,\n",
        "          start_train=2000, train_interval=50,\n",
        "          eval_interval=2000, snapshot_interval=10000, path=None):\n",
        "    \n",
        "    target_actor = copy.deepcopy(agent.actor)\n",
        "    target_critic = copy.deepcopy(agent.critic)\n",
        "    \n",
        "    # freeze target networks\n",
        "    for p in target_actor.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    for p in target_critic.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    actor_optim = Adam(agent.actor.parameters(), lr=actor_lr)\n",
        "    critic_optim = Adam(agent.critic.parameters(), lr=critic_lr)\n",
        "    \n",
        "    if path is not None:\n",
        "        recover_snapshot(path, agent.actor, agent.critic,\n",
        "                   target_actor, target_critic,\n",
        "                   actor_optim, critic_optim,\n",
        "                   device=device\n",
        "                  )\n",
        "        # load snapshot\n",
        "    \n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "    ctrl_range = env.action_space.high[0]\n",
        "    \n",
        "    replay_buf = ReplayBuffer(obs_dim, act_dim, buffer_size)\n",
        "    \n",
        "    save_path = './snapshots/'\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    \n",
        "    test_env = copy.deepcopy(env)\n",
        "\n",
        "    # main loop\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    step_count = 0\n",
        "    ep = 0\n",
        "    for t in range(num_updates + 1):\n",
        "        if t < init_buffer:\n",
        "            # perform random action until we collect sufficiently many samples\n",
        "            # this is for exploration purpose\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # executes noisy action\n",
        "            # a_t = \\pi(s_t) + N(0, \\sigma^2)\n",
        "            action = agent.act(obs) + noise_std * np.random.randn(act_dim)\n",
        "            action = np.clip(action, -ctrl_range, ctrl_range)\n",
        "            \n",
        "        next_obs, rew, done, _ = env.step(action)\n",
        "        step_count += 1\n",
        "        if step_count == ep_len:\n",
        "            # if the next_state is not terminal but done is set to True by gym env wrapper\n",
        "            done = False\n",
        "            \n",
        "        replay_buf.append(obs, action, next_obs, rew, done)\n",
        "        obs = next_obs\n",
        "        \n",
        "        if done == True or step_count == ep_len:\n",
        "            # reset environment if current environment reaches a terminal state \n",
        "            # or step count reaches predefined length\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            step_count = 0\n",
        "            ep += 1\n",
        "\n",
        "        if t % eval_interval == 0:\n",
        "            avg_score = evaluate(agent, test_env, num_episodes=5)\n",
        "            print('[iter {} / ep {}] average score = {:.4f} (over 5 episodes)'.format(t, ep, avg_score))\n",
        "   \n",
        "        if t > start_train and t % train_interval == 0:\n",
        "            # start training after fixed number of steps\n",
        "            # this may mitigate overfitting of networks to the \n",
        "            # small number of samples collected during the initial stage of training\n",
        "            for _ in range(train_interval):\n",
        "                update(agent,\n",
        "                       replay_buf,\n",
        "                       gamma,\n",
        "                       actor_optim,\n",
        "                       critic_optim,\n",
        "                       target_actor,\n",
        "                       target_critic,\n",
        "                       tau,\n",
        "                       batch_size\n",
        "                      )\n",
        "        if t % snapshot_interval == 0:\n",
        "            snapshot_path = save_path + 'iter{}_'.format(t)\n",
        "            # save weight & training progress\n",
        "            save_snapshot(snapshot_path, agent.actor, agent.critic,\n",
        "                          target_actor, target_critic,\n",
        "                          actor_optim, critic_optim)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWwB76w-ZePz"
      },
      "source": [
        "# 4. Let's test the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "0vdCRnK8ZeP0",
        "outputId": "7a179d9a-92f0-4886-f050-8d3f8af3427e"
      },
      "source": [
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "ctrl_range = env.action_space.high[0]\n",
        "\n",
        "print('observation space dim : {} / action space dim : {}'.format(obs_dim, act_dim))\n",
        "print('ctrl range : ', ctrl_range)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DependencyNotInstalled",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#   therefore, pygame is a necessary import for the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpygame\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgfxdraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0b4a5106181e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLanderContinuous-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobs_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mact_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mctrl_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Assume it's a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0menv_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"render_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipedal_walker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBipedalWalker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBipedalWalkerHardcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar_racing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarRacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlunar_lander\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLunarLander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise DependencyNotInstalled(\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;34m\"pygame is not installed, run `pip install gym[box2d]`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gym[box2d]`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu6VftswZeP0"
      },
      "source": [
        "agent = DDPGAgent(obs_dim=obs_dim, act_dim=act_dim, ctrl_range=ctrl_range, hidden1=256, hidden2=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYwH-XeyZeP0"
      },
      "source": [
        "gamma = 0.99\n",
        "actor_lr = 1e-4\n",
        "critic_lr = 1e-3\n",
        "tau = 1e-3\n",
        "noise_std = 0.1\n",
        "ep_len = 1000\n",
        "num_updates = 500000\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJe2AmZZeP1"
      },
      "source": [
        "train(agent, env, gamma,\n",
        "      actor_lr, critic_lr, tau, noise_std,\n",
        "      ep_len, num_updates, batch_size,\n",
        "      init_buffer=5000, buffer_size=100000,\n",
        "      start_train=2000, train_interval=50,\n",
        "      eval_interval=5000\n",
        "     )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDxfKC3tZeP1"
      },
      "source": [
        "# 5. Watch the trained agent!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTFs97dPb8n5"
      },
      "source": [
        "if colab:\n",
        "    import gym\n",
        "    from gym.wrappers import Monitor\n",
        "    import glob\n",
        "    import io\n",
        "    import base64\n",
        "    from IPython.display import HTML\n",
        "    from pyvirtualdisplay import Display\n",
        "    from IPython import display as ipythondisplay\n",
        "\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "\n",
        "    def show_video():\n",
        "      mp4list = glob.glob('video/*.mp4')\n",
        "      if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "      else: \n",
        "        print(\"Could not find video\")\n",
        "        \n",
        "\n",
        "    def wrap_env(env):\n",
        "      env = Monitor(env, './video', force=True)\n",
        "      return env\n",
        "\n",
        "    env = wrap_env(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC5Z6NI0ZeP1"
      },
      "source": [
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "if colab:\n",
        "  env = wrap_env(env)\n",
        "obs = env.reset()\n",
        "done = False\n",
        "score = 0.\n",
        "\n",
        "while not done:\n",
        "    env.render(mode='human')\n",
        "    obs, rew, done, _ = env.step(agent.act(obs))\n",
        "    score += rew\n",
        "print('score : ', score)\n",
        "env.close()\n",
        "\n",
        "if colab:\n",
        "    show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2cVm6FzyM6G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
