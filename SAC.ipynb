{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "sac.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/SNU/blob/main/SAC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ8H7gUejpM8"
      },
      "source": [
        "If you run in jupyter, turn \n",
        "\n",
        "```\n",
        "colab = False\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gLAnNFgjns1",
        "outputId": "eeb6a142-27b9-42d0-bdca-e2d31439502a"
      },
      "source": [
        "colab = True\n",
        "if colab:\n",
        "    !pip install gym==0.21 pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !apt-get update > /dev/null 2>&1\n",
        "    !apt-get install cmake > /dev/null 2>&1\n",
        "    !pip install --upgrade setuptools 2>&1\n",
        "    !pip install ez_setup > /dev/null 2>&1\n",
        "    !pip3 install box2d-py\n",
        "    !pip3 install gym[Box_2D]\n",
        "    !pip3 install pybullet --upgrade"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (65.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "\u001b[33mWARNING: gym 0.21.0 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[Box_2D]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[Box_2D]) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_jHwIghjorZ",
        "outputId": "774f13ee-5c5c-4231-db74-de11fe8b1844"
      },
      "source": [
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/Colab\\ Notebooks/rl-master/rl-master/day4/sac\n",
        "    !ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/rl-master/rl-master/day4/sac'\n",
            "/content\n",
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftguFIU0xJaL"
      },
      "source": [
        "if colab:\n",
        "    import gym\n",
        "    from gym.wrappers import Monitor\n",
        "    import glob\n",
        "    import io\n",
        "    import base64\n",
        "    from IPython.display import HTML\n",
        "    from pyvirtualdisplay import Display\n",
        "    from IPython import display as ipythondisplay\n",
        "\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "\n",
        "    def show_video():\n",
        "      mp4list = glob.glob('video/*.mp4')\n",
        "      if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "      else: \n",
        "        print(\"Could not find video\")\n",
        "        \n",
        "\n",
        "    def wrap_env(env):\n",
        "      env = Monitor(env, './video', force=True)\n",
        "      return env"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS-J14ZNjm3s"
      },
      "source": [
        "# SAC Practice\n",
        "\n",
        "Remind : Key elements of SAC\n",
        "\n",
        "\n",
        "*   Max-entropy MDP setting\n",
        "*   Soft actor improvement with KL-divergence\n",
        "*   Reparameterization trick\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl43cHUvjm3u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "ef5ff59e-b04e-4713-abc0-d38bb2ebfbc4"
      },
      "source": [
        "import time\n",
        "import csv\n",
        "import gym\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Independent\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from utils import *\n",
        "from buffer import *\n",
        "\n",
        "import pybullet_envs"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3898927f7097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqe0Lvc2jm3v"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('current device =', device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYUYp3gjm3w"
      },
      "source": [
        "# 0. Define Q-network & policy-network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLntOL2ijm3w"
      },
      "source": [
        "##################################################\n",
        "##  Policy network with multi-layer perceptron  ##\n",
        "##################################################\n",
        "\n",
        "# Input - |S|\n",
        "# Output - normal distribution of size |A|\n",
        "\n",
        "class SACActor(nn.Module):\n",
        "    def __init__(self, dimS, dimA, hidden1, hidden2, ctrl_range):\n",
        "        super(SACActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(dimS, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden2, dimA)\n",
        "        self.fc4 = nn.Linear(hidden2, dimA)\n",
        "\n",
        "        self.ctrl_range = ctrl_range\n",
        "\n",
        "    def forward(self, state, eval=False, with_log_prob=False):\n",
        "        # Network architecture!\n",
        "        # We will use gaussian policy\n",
        "        #                   -> fc3 -> mu\n",
        "        # s -> fc1 -> fc2 <\n",
        "        #                   -> fc4 -> log(sigma)\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mu = self.fc3(x)\n",
        "        log_sigma = self.fc4(x)\n",
        "        \n",
        "        # clip value of log_sigma, as was done in Haarnoja's implementation of SAC:\n",
        "        # https://github.com/haarnoja/sac.git\n",
        "        log_sigma = torch.clamp(log_sigma, -20.0, 2.0)\n",
        "        \n",
        "        # Build normal distribution with parameters from layer\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        distribution = Independent(Normal(mu, sigma), 1)\n",
        "\n",
        "        if not eval:\n",
        "            # use rsample() instead of sample(), for reparameterization trick\n",
        "            u = distribution.rsample()\n",
        "            if with_log_prob:\n",
        "                log_prob = distribution.log_prob(u)\n",
        "                log_prob -= 2.0 * torch.sum((np.log(2.0) + 0.5 * np.log(self.ctrl_range) - u - F.softplus(-2.0 * u)), dim=1)\n",
        "            else:\n",
        "                log_prob = None\n",
        "        # Give deterministic policy (centered at mu) when evaluation\n",
        "        else:\n",
        "            u = mu\n",
        "            log_prob = None\n",
        "            \n",
        "        # apply tanh so that the resulting action lies in (-1, 1)^D\n",
        "        # Reformulated into squashed gaussian policy\n",
        "        a = self.ctrl_range * torch.tanh(u)\n",
        "\n",
        "        return a, log_prob\n",
        "    \n",
        "\n",
        "##################################################\n",
        "##  Critic network with multi-layer perceptron  ##\n",
        "##################################################\n",
        "\n",
        "# Input - |S|+|A|\n",
        "# Output - single value\n",
        "\n",
        "class DoubleCritic(nn.Module):\n",
        "    # Retain double network - Idea from TD3\n",
        "    def __init__(self, dimS, dimA, hidden1, hidden2):\n",
        "        super(DoubleCritic, self).__init__()    \n",
        "        # Q1\n",
        "        self.fc1 = nn.Linear(dimS + dimA, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, 1)\n",
        "        \n",
        "        # Q2\n",
        "        self.fc4 = nn.Linear(dimS + dimA, hidden1)\n",
        "        self.fc5 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc6 = nn.Linear(hidden2, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        \n",
        "        # Q1\n",
        "        x1 = F.relu(self.fc1(x))\n",
        "        x1 = F.relu(self.fc2(x1))\n",
        "        x1 = self.fc3(x1)\n",
        "        \n",
        "        # Q2\n",
        "        x2 = F.relu(self.fc4(x))\n",
        "        x2 = F.relu(self.fc5(x2))\n",
        "        x2 = self.fc6(x2)\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KQuqaRjm3x"
      },
      "source": [
        "# 1. Define SAC agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S64TpUdbjm3x"
      },
      "source": [
        "#class SACAgent:\n",
        "    def __init__(self,\n",
        "                 dimS,\n",
        "                 dimA,\n",
        "                 ctrl_range,\n",
        "                 gamma=0.99,\n",
        "                 pi_lr=1e-4,\n",
        "                 q_lr=1e-3,\n",
        "                 polyak=1e-3,\n",
        "                 alpha=0.2,\n",
        "                 hidden1=256,\n",
        "                 hidden2=256,\n",
        "                 buffer_size=1000000,\n",
        "                 batch_size=128,\n",
        "                 device='cpu',\n",
        "                 render=False):\n",
        "\n",
        "        self.dimS = dimS\n",
        "        self.dimA = dimA\n",
        "        self.ctrl_range = ctrl_range\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.pi_lr = pi_lr\n",
        "        self.q_lr = q_lr\n",
        "        self.polyak = polyak\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # networks definition\n",
        "        # pi : actor network, Q : 2 critic network\n",
        "        self.pi = SACActor(dimS, dimA, hidden1, hidden2, ctrl_range).to(device)\n",
        "        self.Q = DoubleCritic(dimS, dimA, hidden1, hidden2).to(device)\n",
        "\n",
        "        # target networks\n",
        "        self.target_Q = copy.deepcopy(self.Q).to(device)\n",
        "        freeze(self.target_Q)\n",
        "\n",
        "        self.buffer = ReplayBuffer(dimS, dimA, limit=buffer_size)\n",
        "\n",
        "        self.Q_optimizer = Adam(self.Q.parameters(), lr=self.q_lr)\n",
        "        self.pi_optimizer = Adam(self.pi.parameters(), lr=self.pi_lr)\n",
        "\n",
        "        self.device = device\n",
        "        self.render = render\n",
        "\n",
        "        return\n",
        "    \n",
        "    def act(self, state, eval=False):\n",
        "\n",
        "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action, _ = self.pi(state, eval=eval, with_log_prob=False)\n",
        "        action = action.cpu().detach().numpy()\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def target_update(self):\n",
        "\n",
        "        for params, target_params in zip(self.Q.parameters(), self.target_Q.parameters()):\n",
        "            target_params.data.copy_(self.polyak * params.data + (1.0 - self.polyak) * target_params.data)\n",
        "\n",
        "        return\n",
        "    \n",
        "    def save_model(self, path):\n",
        "        print('adding checkpoints...')\n",
        "        checkpoint_path = path + 'model.pth.tar'\n",
        "        torch.save(\n",
        "                    {'actor': self.pi.state_dict(),\n",
        "                     'critic': self.Q.state_dict(),\n",
        "                     'target_critic': self.target_Q.state_dict(),\n",
        "                     'actor_optimizer': self.pi_optimizer.state_dict(),\n",
        "                     'critic_optimizer': self.Q_optimizer.state_dict()\n",
        "                    },\n",
        "                    checkpoint_path)\n",
        "\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fswi1jUrjm3z"
      },
      "source": [
        "# 2. Implement one-step param update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7B56Nlsjm3z"
      },
      "source": [
        "def update(agent, batch):\n",
        "    # Upload batch to GPU\n",
        "    obs_batch = torch.tensor(batch.obs, dtype=torch.float).to(device)\n",
        "    act_batch = torch.tensor(batch.act, dtype=torch.float).to(device)\n",
        "    next_obs_batch = torch.tensor(batch.next_obs, dtype=torch.float).to(device)\n",
        "    rew_batch = torch.tensor(batch.rew, dtype=torch.float).to(device)\n",
        "    done_batch = torch.tensor(batch.done, dtype=torch.float).to(device)\n",
        "    masks = torch.tensor([1.]).to(device) - done_batch\n",
        "    \n",
        "    #########################\n",
        "    ##    Critic Update    ##\n",
        "    #########################\n",
        "    # Build Bellman target\n",
        "    with torch.no_grad():\n",
        "        # Get action with log(pi(a|s)) (also gradient)\n",
        "        next_actions, log_probs = agent.pi(next_obs_batch, with_log_prob=True)\n",
        "        \n",
        "        # To calculate TQ, we need Q(s',pi(s'))\n",
        "        target_q1, target_q2 = agent.target_Q(next_obs_batch, next_actions)\n",
        "        \n",
        "        # To mitigate overestimation! - Idea from TD3\n",
        "        target_q = torch.min(target_q1, target_q2)\n",
        "        \n",
        "        # TQ^pi = r + gamma [ Q(s',pi(s')) - alpha H(pi(s')) ]\n",
        "        # Recall : H = sum[ -P(X) * log(P(x)) ] = E [ -log(P(x)) ]\n",
        "        # TODO : Make target Q value!\n",
        "        TQ = rew_batch + agent.gamma * masks * (target_q - agent.alpha * log_probs)\n",
        "\n",
        "    # Calculate MSELoss\n",
        "    Q1, Q2 = agent.Q(obs_batch, act_batch)\n",
        "    Q_loss1 = torch.mean((Q1 - TQ)**2)\n",
        "    Q_loss2 = torch.mean((Q2 - TQ)**2)\n",
        "    Q_loss = Q_loss1 + Q_loss2\n",
        "\n",
        "    # Gradient descent\n",
        "    agent.Q_optimizer.zero_grad()\n",
        "    Q_loss.backward()\n",
        "    agent.Q_optimizer.step()\n",
        "    \n",
        "    ########################\n",
        "    ##    Actor Update    ##\n",
        "    ########################\n",
        "    actions, log_probs = agent.pi(obs_batch, with_log_prob=True)\n",
        "    \n",
        "    freeze(agent.Q)\n",
        "    q1, q2 = agent.Q(obs_batch, actions)\n",
        "    q = torch.min(q1, q2)\n",
        "\n",
        "    # TODO: build soft actor loss\n",
        "    # Hint : agent.alpha is alpha value in loss!\n",
        "    pi_loss = torch.mean(agent.alpha * log_probs - q)\n",
        "    \n",
        "    # Gradient ascent\n",
        "    agent.pi_optimizer.zero_grad()\n",
        "    pi_loss.backward()\n",
        "    agent.pi_optimizer.step()\n",
        "    \n",
        "    ####################################\n",
        "    ##    Soft Target Critic Update    #\n",
        "    ####################################\n",
        "    unfreeze(agent.Q)\n",
        "    agent.target_update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhW0rX6Rjm30"
      },
      "source": [
        "# 3. Putting these together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QknuOyCfjm31"
      },
      "source": [
        "def run_sac(\n",
        "            agent,\n",
        "            env_id,\n",
        "            max_iter=1e6,\n",
        "            eval_interval=2000,\n",
        "            start_train=10000,\n",
        "            train_interval=50,\n",
        "            fill_buffer=20000,\n",
        "            truncate=1000,\n",
        "            ):\n",
        "\n",
        "    params = locals()\n",
        "\n",
        "    max_iter = int(max_iter)\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    if truncate is not None:\n",
        "        max_ep_len = truncate\n",
        "\n",
        "    set_log_dir(env_id)\n",
        "    \n",
        "    # Logging & Saving Weights\n",
        "    num_checkpoints = 5\n",
        "    checkpoint_interval = max_iter // (num_checkpoints - 1)\n",
        "    current_time = time.strftime(\"%m%d-%H%M%S\")\n",
        "    train_log = open('./train_log/' + env_id + '/SAC_' + current_time + '.csv',\n",
        "                     'w', encoding='utf-8', newline='')\n",
        "\n",
        "    path = './eval_log/' + env_id + '/SAC_' + current_time\n",
        "    eval_log = open(path + '.csv', 'w', encoding='utf-8', newline='')\n",
        "\n",
        "    train_logger = csv.writer(train_log)\n",
        "    eval_logger = csv.writer(eval_log)\n",
        "\n",
        "    with open(path + '.txt', 'w') as f:\n",
        "        for key, val in params.items():\n",
        "            print(key, '=', val, file=f)\n",
        "\n",
        "    ##############################\n",
        "    ##    Main training loop    ##\n",
        "    ##############################\n",
        "    obs = env.reset()\n",
        "    step_count, ep_reward = 0, 0\n",
        "    start = time.time()\n",
        "    \n",
        "    for t in range(max_iter + 1):\n",
        "        # Rollout agent to fill in replay buffer\n",
        "        if t < fill_buffer:\n",
        "            # For early stage of training, use random agent to promote exploration\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = agent.act(obs)\n",
        "\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "        step_count += 1\n",
        "\n",
        "        if step_count == max_ep_len:\n",
        "            done = False\n",
        "\n",
        "        agent.buffer.append(obs, action, next_obs, reward, done)\n",
        "\n",
        "        obs = next_obs\n",
        "        ep_reward += reward\n",
        "        \n",
        "        # Reset environment if trajectory ends\n",
        "        if done or (step_count == max_ep_len):\n",
        "            train_logger.writerow([t, ep_reward])\n",
        "            obs = env.reset()\n",
        "            step_count, ep_reward = 0, 0\n",
        "        \n",
        "        # Actor-Critic\n",
        "        if (t >= start_train) and (t % train_interval == 0):\n",
        "            # Iterate sampling batch and updating actor-critic\n",
        "            for _ in range(train_interval):\n",
        "                batch = agent.buffer.sample_batch(batch_size=batch_size)\n",
        "                update(agent, batch)\n",
        "        \n",
        "        # Evaluate agent\n",
        "        if t % eval_interval == 0:\n",
        "            eval_score = eval_agent(agent, env_id, render=False)\n",
        "            log = [t, eval_score]\n",
        "            print('step {} : {:.4f}'.format(t, eval_score))\n",
        "            eval_logger.writerow(log)\n",
        "            agent.save_model('./checkpoints/' + env_id + '/sac_{}th_iter_'.format(t))\n",
        "\n",
        "    train_log.close()\n",
        "    eval_log.close()\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8w3dg_gjm32"
      },
      "source": [
        "# 4. Let's train our agent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxaZWNYUjm32"
      },
      "source": [
        "### Hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNnA9yCjm32"
      },
      "source": [
        "# Use continuous control!\n",
        "env_id = 'HalfCheetahBulletEnv-v0'\n",
        "env = gym.make(env_id)\n",
        "dimS, dimA, ctrl_range, max_ep_len = get_env_spec(env)\n",
        "truncate = 1000\n",
        "max_iter = 5e5\n",
        "eval_interval = 5000\n",
        "render = False\n",
        "tau = 5e-3\n",
        "lr = 3e-4\n",
        "hidden1 = 256\n",
        "hidden2 = 256\n",
        "train_interval = 50\n",
        "start_train = 1e4\n",
        "fill_buffer = 2e4\n",
        "gamma = 0.99\n",
        "alpha = 0.01\n",
        "buffer_size = 1e6\n",
        "batch_size = 4000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THQO8s7Gjm32"
      },
      "source": [
        "### Setup environment and agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO6A6dBGjm33"
      },
      "source": [
        "# You can try one of these to perform \n",
        "# HopperBulletEnv-v0\n",
        "# HumanoidBulletEnv-v0\n",
        "# Walker2DBulletEnv-v0\n",
        "# HalfCheetahBulletEnv-v0\n",
        "\n",
        "env = gym.make('HalfCheetahBulletEnv-v0')\n",
        "get_env_spec(env)\n",
        "\n",
        "# Let's watch robotics environment!\n",
        "if colab:\n",
        "    env = wrap_env(env)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "score = 0.\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    obs, rew, done, _ = env.step(env.action_space.sample())\n",
        "    score += rew\n",
        "    \n",
        "env.close()\n",
        "print('score : ', score)\n",
        "\n",
        "if colab:\n",
        "    show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nErNU89ijm33"
      },
      "source": [
        "# Instantize agent\n",
        "agent = SACAgent(\n",
        "                 dimS,\n",
        "                 dimA,\n",
        "                 ctrl_range,\n",
        "                 gamma=gamma,\n",
        "                 pi_lr=lr,\n",
        "                 q_lr=lr,\n",
        "                 polyak=tau,\n",
        "                 alpha=alpha,\n",
        "                 hidden1=hidden1,\n",
        "                 hidden2=hidden2,\n",
        "                 buffer_size=int(buffer_size),\n",
        "                 batch_size=batch_size,\n",
        "                 device=device,\n",
        "                 render=render\n",
        "                 )\n",
        "\n",
        "# Load pretrained model\n",
        "# load_model(agent, path='./checkpoints/'+env_id+'/sac_baseline_model.pth.tar', device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KV9UWSajm33"
      },
      "source": [
        "### Run experiment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "r1UQiSkcjm33"
      },
      "source": [
        "run_sac(\n",
        "        agent,\n",
        "        env_id,\n",
        "        max_iter=max_iter,\n",
        "        eval_interval=eval_interval,\n",
        "        start_train=start_train,\n",
        "        train_interval=train_interval,\n",
        "        fill_buffer=fill_buffer,\n",
        "        truncate=truncate,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKH9xf-xaciS"
      },
      "source": [
        "Save model trained so far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg38-pKPabAj"
      },
      "source": [
        "agent.save_model('./checkpoints/' + env_id + '/sac_final_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY0yUx3zjm34"
      },
      "source": [
        "# 5. Watch the trained agent!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPtAACW9jm34"
      },
      "source": [
        "# For calling the weight and re-use\n",
        "env_id = 'HalfCheetahBulletEnv-v0'\n",
        "\n",
        "env = gym.make(env_id)\n",
        "dimS, dimA, ctrl_range, max_ep_len = get_env_spec(env)\n",
        "if colab:\n",
        "    env = wrap_env(env)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "score = 0.\n",
        "\n",
        "agent = SACAgent(dimS, dimA, ctrl_range)\n",
        "load_model(agent, path='./checkpoints/'+env_id+'/sac_final_model.pth.tar', device=device)\n",
        "# load_model(agent, path='./checkpoints/'+env_id+'/sac_expert_model.pth.tar', device=device)\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    obs, rew, done, _ = env.step(agent.act(obs))\n",
        "    score += rew\n",
        "    \n",
        "env.close()\n",
        "print('score : ', score)\n",
        "\n",
        "if colab:\n",
        "    show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}